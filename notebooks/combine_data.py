#%% [markdown]
# # Combine and Clean Data
# This is just a notebook file I can use to merge data generated by seperate
# scripts into fewer data files. I have to do this fairly often so it makes
# sense to keep the code somewhere I think
# %%
import pandas as pd
from glob import glob
#%% [markdown]
# ## $I_{\pm}$ for $k=5$ one dimensional CAs
# each row is outputed in parallel so they have to be combined to be useful
# %%
dfs = []
for file in glob('../data/k5/pid/raw_ipm/*_ipm.csv'):
    rule = int(file.split('/')[-1].split('_')[0])
    print('Adding rule ', rule)
    df_row = pd.read_csv(file, index_col = 0)
    df_row['rule'] = rule
    dfs.append(df_row)

print('Merging dfs')
big_ipm_df = pd.concat(dfs)
big_ipm_df.to_csv('../data/k5/pid/ipm_ugly.csv', index=None)
# %% [markdown]
# ### change column names
# the atom names that come out of `dit` are very ugly. lets change them with
# this function I wrote.
# %%
import re

def pretty_labels_map(atom_labels):
    """
    transforms all of these crazy tuples into the notation used in williams and
    beer I_min PID
    """
    rename_map = {}
    for label in atom_labels:
        new_label = str(label)

        # eliminate commas and spaces
        new_label = new_label.replace(',', '')
        new_label = new_label.replace(' ', '')

        # replace braces
        new_label = new_label.replace('(', '{')
        new_label = new_label.replace(')', '}')

        # separate digits with colons
        while re.search(r'\d\d', new_label):
            new_label = re.sub(r'(\d)(\d)', r'\1:\2', new_label)

        # put them in a map
        rename_map[label] = new_label[1:-1]
    
    return rename_map
# %%

ipm_df = pd.read_csv('../data/k5/pid/ipm_ugly.csv')

lab_map = pretty_labels_map(ipm_df.columns)
lab_map['rule'] = 'rule'

ipm_df = ipm_df.rename(columns = lab_map)
ipm_df.to_csv('../data/k5/pid/ipm.csv')
# %% [markdown]
# ## Canalization calculations
# 
# I have the canalization stuff in files for each lambda value. Thats fine but
# I really need to combine them into one data frame to compare stuff more easily

#%%
dfs = []
for file in glob('../data/k5/stats/k5_cana_lambda*.csv'):
    l_val = file.split('/')[-1].split('_')[-1].split('.')[0]
    print('Loading lambda =', l_val)
    df_chunk = pd.read_csv(file, index_col = 0)
    df_chunk['lambda'] = l_val

    dfs.append(df_chunk)
   
print('Merging')
df_cana = pd.concat(dfs)
df_cana.to_csv('../data/k5/stats/k5_cana.csv')
# %% [markdown]
# ## $I_{min}$ for $k=5$ for one-dimensional CA
#
# This is basically the same proceedure as before. we need to get all of the
# single rows into one df and rename the columns
# %%
dfs = []
for file in glob('../data/k5/pid/raw_imin/*_imin.csv'):
    rule = int(file.split('/')[-1].split('_')[0])
    print('Adding rule ', rule)
    df_row = pd.read_csv(file, index_col = 0)
    df_row['rule'] = rule
    dfs.append(df_row)

print('Merging dfs')
big_ipm_df = pd.concat(dfs)
big_ipm_df.to_csv('../data/k5/pid/imin_ugly.csv', index=None)

#%%
imin_df = pd.read_csv('../data/k5/pid/imin_ugly.csv')

lab_map = pretty_labels_map(imin_df.columns)
lab_map['rule'] = 'rule'

imin_df = imin_df.rename(columns = lab_map)
imin_df.to_csv('../data/k5/pid/imin.csv')
# %% [markdown]
# # O-information
#
# The O-information calculations take a long time so I've started preserving 
# all of the old ones and just adding a file with new changes. these need to be 
# merged whenever new calculations are done.

# %%
o_info_new = pd.read_csv('../data/k5/stats/o_information_new.csv', index_col=0)
o_info_old = pd.read_csv('../data/k5/stats/o_information.csv', index_col=0)

o_info_all = o_info_old.merge(o_info_new, how='outer', on='rule')

print('Old df rows:', o_info_old.shape[0])
print('New df rows:', o_info_new.shape[0])
print('Sum of Rows:', o_info_old.shape[0] + o_info_new.shape[0])
print('New df rows:', o_info_all.shape[0])

o_info_all.to_csv('../data/k5/stats/o_information.csv')
# %% [markdown]
# # Directed measures
#
# Finally we can combine transfer entropy measurements and active information
# storage because they are _sort of_ the same thing. We use them in the same way
# at least.
# %%
te_data = pd.read_csv('../data/k5/stats/te_rules.csv', index_col=0)
ais_data = pd.read_csv('../data/k5/stats/ais_rules.csv', index_col=0)

directed = te_data.merge(ais_data, on='rule')

print('TE shape:', te_data.shape)
print('AIS shape:', ais_data.shape)
print('Merged shape:', directed.shape)

directed.to_csv('../data/k5/stats/directed.csv')
# %% [markdown]
# ## Dynamics data
#
# Here we will combine all of the data living in 'data/k5/attractors/' into
# something more useful.

# %%
dfs = []
for fin in glob('../data/k5/attractors/lambda*.csv'):
    lamb = fin.split('_')[1]
    # load the dynamics data
    dyn = pd.read_csv(fin,
                      index_col=0)
    # have to do some really dumb conversion
    dyn['rule'] = dyn['rule'].astype(int)

    # lets just average over transients and periods here
    trans = dyn[dyn['measure'] == 'transient'].copy().drop('measure', axis=1)
    trans['transient'] = trans['value']
    trans = trans[['rule', 'trial', 'transient']]
    trans['unq_ind'] = range(0, trans.shape[0])
    print(trans.shape)

    peri = dyn[dyn['measure'] == 'period'].copy().drop('measure', axis=1)
    peri['period'] = peri['value']
    peri = peri[['rule', 'trial', 'period']]
    peri['unq_ind'] = range(0, peri.shape[0])
    print(peri.shape)

    # combine and clean. get mean of feautres by rule
    dyn1 = pd.merge(peri, trans, how='left', on=['unq_ind'])
    dyn1['lambda'] = lamb
    dyn1['rule'] = dyn1['rule_x']
    dyn1['period_transient'] = (dyn1['period'] + dyn1['transient']).fillna(64000)
    dyn = (dyn1[['lambda', 'rule', 'period_transient', 'period', 'transient']]
           .groupby(['rule', 'lambda'])
           .mean()
           .reset_index())
    
    dfs.append(dyn)

dynamics = pd.concat(dfs)
dynamics.to_csv('../data/k5/stats/dynamics.csv')
# %%
